{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02784ca-c731-4947-9a77-ff2632e6e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model from file\n",
    "pen_to_pixel = load_model(\"pen_to_pixel.keras\")\n",
    "from tensorflow.keras import datasets, layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b34682-9ec3-4c84-8f64-a493a7c54e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676b395e-f575-45f4-b388-4e25ab26a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Scanning dataset...\n",
      "Found 1545923 images across 62 classes\n",
      "Example classes: ['30', '31', '32', '33', '34', '35', '36', '37', '38', '39']\n",
      "Batch image shape: (128, 128, 128, 1)\n",
      "Batch labels: [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "# Acceptable image extensions in SD19 dumps\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\", \".gif\"}\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "def is_image_file(p: str) -> bool:\n",
    "    \"\"\"Check if file is an image (skip .mit and other non-images).\"\"\"\n",
    "    return Path(p).suffix.lower() in IMG_EXTS\n",
    "\n",
    "def scan_dataset(by_class_dir: str) -> Tuple[List[str], List[int], Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Scan SD19 by_class directory structure:\n",
    "      by_class/\n",
    "        <class_name>/\n",
    "          hsf_1/*.png\n",
    "          hsf_2/*.png\n",
    "          ...\n",
    "    Returns:\n",
    "      filepaths: list of image file paths\n",
    "      labels:    list of integer labels\n",
    "      label2idx: mapping {label_str -> int}\n",
    "      idx2label: reverse mapping\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    str_labels = []\n",
    "\n",
    "    # Top-level label folders (e.g., \"30\", \"7a\", \"A\", etc.)\n",
    "    label_dirs = sorted([d for d in glob.glob(os.path.join(by_class_dir, \"*\")) if os.path.isdir(d)])\n",
    "\n",
    "    # Build label map (alphabetical order for consistency)\n",
    "    label_names = [os.path.basename(d) for d in label_dirs]\n",
    "    label2idx = {lab: i for i, lab in enumerate(label_names)}\n",
    "    idx2label = {i: lab for lab, i in label2idx.items()}\n",
    "\n",
    "    # Collect image paths + labels\n",
    "    for lab_dir in label_dirs:\n",
    "        lab = os.path.basename(lab_dir)\n",
    "        for p in glob.glob(os.path.join(lab_dir, \"**\", \"*\"), recursive=True):\n",
    "            if os.path.isfile(p) and is_image_file(p):\n",
    "                filepaths.append(p)\n",
    "                str_labels.append(lab)\n",
    "\n",
    "    # Convert labels to integer indices\n",
    "    int_labels = [label2idx[s] for s in str_labels]\n",
    "\n",
    "    return filepaths, int_labels, label2idx, idx2label\n",
    "\n",
    "def load_and_preprocess_image(path: tf.Tensor, img_size=(128, 128)) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Load one image, convert to grayscale, resize, scale to [0,1].\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img, channels=1, expand_animations=False)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "# =========================\n",
    "# Example Usage\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_DIR = \"/home/mudda/Downloads/by_class\"   # <- change to your SD19 path\n",
    "    \n",
    "    print(\"ğŸ” Scanning dataset...\")\n",
    "    filepaths, labels, label2idx, idx2label = scan_dataset(DATASET_DIR)\n",
    "    print(f\"Found {len(filepaths)} images across {len(label2idx)} classes\")\n",
    "    print(\"Example classes:\", list(label2idx.keys())[:10])  # first 10 class names\n",
    "\n",
    "    # Convert to tf.data.Dataset\n",
    "    paths_ds = tf.data.Dataset.from_tensor_slices(filepaths)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    dataset = tf.data.Dataset.zip((paths_ds, labels_ds))\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda p, y: (load_and_preprocess_image(p), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.shuffle(10000).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Test: fetch one batch\n",
    "    for images, labs in dataset.take(1):\n",
    "        print(\"Batch image shape:\", images.shape)\n",
    "        print(\"Batch labels:\", labs[:10].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07aedab-ed59-4672-8981-62b361d9cfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Scanning dataset at: /home/mudda/Downloads/by_class\n",
      "Found 1545923 images across 62 classes.\n",
      "Example classes: ['30', '31', '32', '33', '34', '35', '36', '37', '38', '39']\n",
      "Train samples: 1314034 | Val samples: 231889\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pen_to_pixel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"pen_to_pixel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_16          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_17          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_18          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_19          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ permute_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,914,944</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,806</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚           \u001b[38;5;34m320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_16          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_17          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_18          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚       \u001b[38;5;34m295,168\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_19          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ permute_4 (\u001b[38;5;33mPermute\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda_4 (\u001b[38;5;33mLambda\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4096\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚     \u001b[38;5;34m8,914,944\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_9 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚     \u001b[38;5;34m1,574,912\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             â”‚        \u001b[38;5;34m31,806\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,911,422</span> (41.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,911,422\u001b[0m (41.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,910,462</span> (41.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,910,462\u001b[0m (41.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m82128/82128\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8233 - loss: 0.5442\n",
      "Epoch 1: val_accuracy improved from None to 0.87075, saving model to pen_to_pixel.keras\n",
      "\u001b[1m82128/82128\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6087s\u001b[0m 74ms/step - accuracy: 0.8528 - loss: 0.4346 - val_accuracy: 0.8707 - val_loss: 0.3732 - learning_rate: 0.0010\n",
      "Epoch 2/3\n",
      "\u001b[1m39675/82128\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:04:03\u001b[0m 91ms/step - accuracy: 0.8657 - loss: 0.3918"
     ]
    }
   ],
   "source": [
    "# sd19_pen_to_pixel.py\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image  # noqa: F401  # (kept so you can sanity-check images if needed)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from tensorflow.keras import mixed_precision\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "# only show warnings and errors\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "DATASET_DIR = \"/home/mudda/Downloads/by_class\"   # <-- change to your SD19 by_class path\n",
    "IMG_SIZE = (128, 128)                            # (H, W)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "VAL_SPLIT = 0.15\n",
    "RANDOM_SEED = 42\n",
    "SHUFFLE_BUFFER = 10_000\n",
    "\n",
    "# Save names\n",
    "BEST_MODEL_PATH = \"pen_to_pixel.keras\"           # best checkpoint (.keras zip)\n",
    "FINAL_MODEL_PATH = \"pen_to_pixel.keras\"    # final save after training\n",
    "LABELMAP_JSON = \"pen_to_pixel_labelmap.json\"     # label maps for inference\n",
    "\n",
    "# Acceptable image extensions in typical SD19 dumps (skip .mit etc.)\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\", \".gif\"}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Reproducibility niceties\n",
    "# =========================\n",
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Dataset utilities\n",
    "# =========================\n",
    "def is_image_file(p: str) -> bool:\n",
    "    return Path(p).suffix.lower() in IMG_EXTS\n",
    "\n",
    "\n",
    "def scan_dataset(by_class_dir: str) -> Tuple[List[str], List[str], Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Scans SD19 by_class directory structure:\n",
    "      by_class/\n",
    "        <class_name_or_id>/\n",
    "          hsf_1/*.png ...\n",
    "          hsf_2/*.png ...\n",
    "    Returns:\n",
    "      filepaths: list of image file paths\n",
    "      str_labels: list of string labels (top-level dir names)\n",
    "      label2idx: mapping {label_str -> int}\n",
    "      idx2label: reverse mapping\n",
    "    \"\"\"\n",
    "    filepaths: List[str] = []\n",
    "    str_labels: List[str] = []\n",
    "\n",
    "    label_dirs = sorted([d for d in glob.glob(os.path.join(by_class_dir, \"*\")) if os.path.isdir(d)])\n",
    "    if not label_dirs:\n",
    "        raise RuntimeError(f\"No class folders found under: {by_class_dir}\")\n",
    "\n",
    "    label_names = [os.path.basename(d) for d in label_dirs]\n",
    "    label2idx = {lab: i for i, lab in enumerate(label_names)}\n",
    "    idx2label = {i: lab for lab, i in label2idx.items()}\n",
    "\n",
    "    for lab_dir in label_dirs:\n",
    "        lab = os.path.basename(lab_dir)\n",
    "        for p in glob.glob(os.path.join(lab_dir, \"**\", \"*\"), recursive=True):\n",
    "            if os.path.isfile(p) and is_image_file(p):\n",
    "                filepaths.append(p)\n",
    "                str_labels.append(lab)\n",
    "\n",
    "    if len(filepaths) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No image files found. Ensure DATASET_DIR points to SD19 'by_class' root \"\n",
    "            \"and that image files (e.g., .png) exist.\"\n",
    "        )\n",
    "\n",
    "    return filepaths, str_labels, label2idx, idx2label\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(path: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Reads an image file, converts to grayscale, resizes to IMG_SIZE, scales to [0,1].\n",
    "    Returns shape (H, W, 1).\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img, channels=1, expand_animations=False)  # grayscale\n",
    "    # Ensure (H, W, 1) static rank\n",
    "    img.set_shape([None, None, 1])\n",
    "    img = tf.image.resize(img, IMG_SIZE, method=tf.image.ResizeMethod.BILINEAR)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def build_dataset(paths: List[str], labels: np.ndarray, training: bool) -> tf.data.Dataset:\n",
    "    ds_paths = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    ds_labels = tf.data.Dataset.from_tensor_slices(labels.astype(np.int32))\n",
    "    ds = tf.data.Dataset.zip((ds_paths, ds_labels))\n",
    "\n",
    "    def _map_fn(p, y):\n",
    "        img = load_and_preprocess_image(p)\n",
    "        return img, y\n",
    "\n",
    "    ds = ds.map(_map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(SHUFFLE_BUFFER, seed=RANDOM_SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Model: CNN + BiLSTM (CRNN)\n",
    "# =========================\n",
    "def build_crnn_model(num_classes: int, input_shape=(128, 128, 1)) -> keras.Model:\n",
    "    \"\"\"\n",
    "    CRNN for single-character classification.\n",
    "    - CNN extracts feature map (B, H', W', C')\n",
    "    - Reshape into a sequence across width -> BiLSTM\n",
    "    - Dense softmax for class logits\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=input_shape, name=\"image\")\n",
    "\n",
    "    # CNN feature extractor\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)  # -> 64x64\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)  # -> 32x32\n",
    "\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)  # -> 16x16\n",
    "\n",
    "    x = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # Keep 16x16 to have a decent sequence length (width=16)\n",
    "\n",
    "    # Prepare sequence: (B, H, W, C) -> (B, W, H*C)\n",
    "    # Use dynamic shape to be safe\n",
    "    x = layers.Permute((2, 1, 3))(x)  # (B, W, H, C)\n",
    "    x = layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2] * tf.shape(t)[3]]))(x)\n",
    "\n",
    "    # RNN\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=False))(x)  # -> (B, 512)\n",
    "\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    pen_to_pixel = keras.Model(inp, out, name=\"pen_to_pixel\")\n",
    "    pen_to_pixel.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return pen_to_pixel\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Train / Eval\n",
    "# =========================\n",
    "def main():\n",
    "    set_all_seeds(RANDOM_SEED)\n",
    "\n",
    "    # Optional GPU memory growth\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 1) Scan dataset\n",
    "    print(f\"ğŸ” Scanning dataset at: {DATASET_DIR}\")\n",
    "    filepaths, str_labels, label2idx, idx2label = scan_dataset(DATASET_DIR)\n",
    "    print(f\"Found {len(filepaths)} images across {len(label2idx)} classes.\")\n",
    "    print(\"Example classes:\", list(label2idx.keys())[:10])\n",
    "\n",
    "    # 2) Convert string labels to class indices\n",
    "    y = np.array([label2idx[s] for s in str_labels], dtype=np.int32)\n",
    "\n",
    "    # 3) Stratified train/val split\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SPLIT, random_state=RANDOM_SEED)\n",
    "    train_idx, val_idx = next(sss.split(filepaths, y))\n",
    "    train_paths = [filepaths[i] for i in train_idx]\n",
    "    val_paths = [filepaths[i] for i in val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    print(f\"Train samples: {len(train_paths)} | Val samples: {len(val_paths)}\")\n",
    "\n",
    "    # 4) Build tf.data pipelines (with shuffle for training)\n",
    "    ds_train = build_dataset(train_paths, y_train, training=True)\n",
    "    ds_val = build_dataset(val_paths, y_val, training=False)\n",
    "\n",
    "    # 5) Build model\n",
    "    num_classes = len(label2idx)\n",
    "    pen_to_pixel = build_crnn_model(num_classes=num_classes, input_shape=(*IMG_SIZE, 1))\n",
    "    pen_to_pixel.summary()\n",
    "\n",
    "    # 6) Callbacks: checkpoint + early stopping + reduce LR\n",
    "    dirname = os.path.dirname(BEST_MODEL_PATH)\n",
    "    if dirname:\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "    ckpt = keras.callbacks.ModelCheckpoint(\n",
    "        BEST_MODEL_PATH,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,  # save full model in .keras (zip) format\n",
    "        mode=\"max\",\n",
    "        verbose=1,\n",
    "    )\n",
    "    early = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=6,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    rlr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # 7) Train\n",
    "    history = pen_to_pixel.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[ckpt, early, rlr],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # 8) Save final (best already saved via checkpoint)\n",
    "    pen_to_pixel.save(FINAL_MODEL_PATH)\n",
    "    print(f\"Saved final model to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "    # 9) Evaluate on validation set\n",
    "    val_metrics = pen_to_pixel.evaluate(ds_val, verbose=1)\n",
    "    print(\"Validation metrics:\", dict(zip(pen_to_pixel.metrics_names, val_metrics)))\n",
    "\n",
    "    # 10) Save label maps (critical for decoding predictions later)\n",
    "    with open(LABELMAP_JSON, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"label2idx\": label2idx,\n",
    "                \"idx2label\": {int(k): v for k, v in idx2label.items()},\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "    print(f\"Saved label maps to {LABELMAP_JSON}\")\n",
    "\n",
    "    # 11) Quick demo predictions on a few validation images\n",
    "    print(\"Demo predictions (first 5 validation images):\")\n",
    "    sample_paths = val_paths[:5]\n",
    "    for p in sample_paths:\n",
    "        pred_idx, conf = predict_single_image(pen_to_pixel, p)\n",
    "        print(f\"{Path(p).name} -> pred={idx2label[pred_idx]}  conf={conf:.3f}\")\n",
    "\n",
    "\n",
    "def predict_single_image(model: keras.Model, path: str) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Loads one image path, returns (pred_class_index, confidence).\n",
    "    \"\"\"\n",
    "    img = load_and_preprocess_image(tf.constant(path))\n",
    "    img = tf.expand_dims(img, axis=0)  # (1, H, W, 1)\n",
    "    probs = model.predict(img, verbose=0)[0]\n",
    "    pred_idx = int(tf.argmax(probs).numpy())\n",
    "    conf = float(tf.reduce_max(probs).numpy())\n",
    "    return pred_idx, conf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291d20b4-50e5-497b-a1e5-aec0d2a0253b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9630fb0-a8b0-4c0b-84cd-6d0234074b0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# take 1 batch from dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mds\u001b[49m.take(\u001b[32m1\u001b[39m):\n\u001b[32m      5\u001b[39m     sample_image = images[\u001b[32m0\u001b[39m].numpy().squeeze()   \u001b[38;5;66;03m# (128,128,1) â†’ (128,128)\u001b[39;00m\n\u001b[32m      6\u001b[39m     sample_label = labels[\u001b[32m0\u001b[39m].numpy()\n",
      "\u001b[31mNameError\u001b[39m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# take 1 batch from dataset\n",
    "for images, labels in ds.take(1):\n",
    "    sample_image = images[0].numpy().squeeze()   # (128,128,1) â†’ (128,128)\n",
    "    sample_label = labels[0].numpy()\n",
    "    break\n",
    "\n",
    "plt.imshow(sample_image, cmap=\"gray\")\n",
    "plt.title(f\"Label: {sample_label}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614ebd8-a27f-413c-913f-b4fe0bde8ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 08:50:49.713007: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-26 08:50:49.747151: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-26 08:50:50.452524: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Scanning dataset at: /home/mudda/Downloads/by_class\n",
      "Found 1545923 images across 62 classes.\n",
      "Example classes: ['30', '31', '32', '33', '34', '35', '36', '37', '38', '39']\n",
      "Train samples: 1314034 | Val samples: 231889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756178466.044252    3583 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1234 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2050, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pen_to_pixel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"pen_to_pixel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_3           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ permute (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,914,944</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,806</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚           \u001b[38;5;34m320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚       \u001b[38;5;34m295,168\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_3           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ permute (\u001b[38;5;33mPermute\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda (\u001b[38;5;33mLambda\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4096\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚     \u001b[38;5;34m8,914,944\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚     \u001b[38;5;34m1,574,912\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             â”‚        \u001b[38;5;34m31,806\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,911,422</span> (41.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,911,422\u001b[0m (41.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,910,462</span> (41.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,910,462\u001b[0m (41.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 08:51:10.828289: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "2025-08-26 08:51:11.190983: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  602/82128\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:50:51\u001b[0m 82ms/step - accuracy: 0.3679 - loss: 2.5143"
     ]
    }
   ],
   "source": [
    "# sd19_pen_to_pixel.py\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image  # noqa: F401  # (kept so you can sanity-check images if needed)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from tensorflow.keras import mixed_precision\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "# only show warnings and errors\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "DATASET_DIR = \"/home/mudda/Downloads/by_class\"   # <-- change to your SD19 by_class path\n",
    "IMG_SIZE = (128, 128)                            # (H, W)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "VAL_SPLIT = 0.15\n",
    "RANDOM_SEED = 42\n",
    "SHUFFLE_BUFFER = 10_000\n",
    "\n",
    "# Save names\n",
    "BEST_MODEL_PATH = \"pen_to_pixel.keras\"           # best checkpoint (.keras zip)\n",
    "FINAL_MODEL_PATH = \"pen_to_pixel.keras\"    # final save after training\n",
    "LABELMAP_JSON = \"pen_to_pixel_labelmap.json\"     # label maps for inference\n",
    "\n",
    "# Acceptable image extensions in typical SD19 dumps (skip .mit etc.)\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\", \".gif\"}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Reproducibility niceties\n",
    "# =========================\n",
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Dataset utilities\n",
    "# =========================\n",
    "def is_image_file(p: str) -> bool:\n",
    "    return Path(p).suffix.lower() in IMG_EXTS\n",
    "\n",
    "\n",
    "def scan_dataset(by_class_dir: str) -> Tuple[List[str], List[str], Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Scans SD19 by_class directory structure:\n",
    "      by_class/\n",
    "        <class_name_or_id>/\n",
    "          hsf_1/*.png ...\n",
    "          hsf_2/*.png ...\n",
    "    Returns:\n",
    "      filepaths: list of image file paths\n",
    "      str_labels: list of string labels (top-level dir names)\n",
    "      label2idx: mapping {label_str -> int}\n",
    "      idx2label: reverse mapping\n",
    "    \"\"\"\n",
    "    filepaths: List[str] = []\n",
    "    str_labels: List[str] = []\n",
    "\n",
    "    label_dirs = sorted([d for d in glob.glob(os.path.join(by_class_dir, \"*\")) if os.path.isdir(d)])\n",
    "    if not label_dirs:\n",
    "        raise RuntimeError(f\"No class folders found under: {by_class_dir}\")\n",
    "\n",
    "    label_names = [os.path.basename(d) for d in label_dirs]\n",
    "    label2idx = {lab: i for i, lab in enumerate(label_names)}\n",
    "    idx2label = {i: lab for lab, i in label2idx.items()}\n",
    "\n",
    "    for lab_dir in label_dirs:\n",
    "        lab = os.path.basename(lab_dir)\n",
    "        for p in glob.glob(os.path.join(lab_dir, \"**\", \"*\"), recursive=True):\n",
    "            if os.path.isfile(p) and is_image_file(p):\n",
    "                filepaths.append(p)\n",
    "                str_labels.append(lab)\n",
    "\n",
    "    if len(filepaths) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No image files found. Ensure DATASET_DIR points to SD19 'by_class' root \"\n",
    "            \"and that image files (e.g., .png) exist.\"\n",
    "        )\n",
    "\n",
    "    return filepaths, str_labels, label2idx, idx2label\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(path: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Reads an image file, converts to grayscale, resizes to IMG_SIZE, scales to [0,1].\n",
    "    Returns shape (H, W, 1).\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img, channels=1, expand_animations=False)  # grayscale\n",
    "    # Ensure (H, W, 1) static rank\n",
    "    img.set_shape([None, None, 1])\n",
    "    img = tf.image.resize(img, IMG_SIZE, method=tf.image.ResizeMethod.BILINEAR)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def build_dataset(paths: List[str], labels: np.ndarray, training: bool) -> tf.data.Dataset:\n",
    "    ds_paths = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    ds_labels = tf.data.Dataset.from_tensor_slices(labels.astype(np.int32))\n",
    "    ds = tf.data.Dataset.zip((ds_paths, ds_labels))\n",
    "\n",
    "    def _map_fn(p, y):\n",
    "        img = load_and_preprocess_image(p)\n",
    "        return img, y\n",
    "\n",
    "    ds = ds.map(_map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(SHUFFLE_BUFFER, seed=RANDOM_SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Model: CNN + BiLSTM (CRNN)\n",
    "# =========================\n",
    "def build_crnn_model(num_classes: int, input_shape=(128, 128, 1)) -> keras.Model:\n",
    "    \"\"\"\n",
    "    CRNN for single-character classification.\n",
    "    - CNN extracts feature map (B, H', W', C')\n",
    "    - Reshape into a sequence across width -> BiLSTM\n",
    "    - Dense softmax for class logits\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=input_shape, name=\"image\")\n",
    "\n",
    "    # CNN feature extractor\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)  # -> 64x64\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)  # -> 32x32\n",
    "\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)  # -> 16x16\n",
    "\n",
    "    x = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # Keep 16x16 to have a decent sequence length (width=16)\n",
    "\n",
    "    # Prepare sequence: (B, H, W, C) -> (B, W, H*C)\n",
    "    # Use dynamic shape to be safe\n",
    "    x = layers.Permute((2, 1, 3))(x)  # (B, W, H, C)\n",
    "    x = layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2] * tf.shape(t)[3]]))(x)\n",
    "\n",
    "    # RNN\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=False))(x)  # -> (B, 512)\n",
    "\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    pen_to_pixel = keras.Model(inp, out, name=\"pen_to_pixel\")\n",
    "    pen_to_pixel.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return pen_to_pixel\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Train / Eval\n",
    "# =========================\n",
    "def main():\n",
    "    set_all_seeds(RANDOM_SEED)\n",
    "\n",
    "    # Optional GPU memory growth\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 1) Scan dataset\n",
    "    print(f\"ğŸ” Scanning dataset at: {DATASET_DIR}\")\n",
    "    filepaths, str_labels, label2idx, idx2label = scan_dataset(DATASET_DIR)\n",
    "    print(f\"Found {len(filepaths)} images across {len(label2idx)} classes.\")\n",
    "    print(\"Example classes:\", list(label2idx.keys())[:10])\n",
    "\n",
    "    # 2) Convert string labels to class indices\n",
    "    y = np.array([label2idx[s] for s in str_labels], dtype=np.int32)\n",
    "\n",
    "    # 3) Stratified train/val split\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SPLIT, random_state=RANDOM_SEED)\n",
    "    train_idx, val_idx = next(sss.split(filepaths, y))\n",
    "    train_paths = [filepaths[i] for i in train_idx]\n",
    "    val_paths = [filepaths[i] for i in val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    print(f\"Train samples: {len(train_paths)} | Val samples: {len(val_paths)}\")\n",
    "\n",
    "    # 4) Build tf.data pipelines (with shuffle for training)\n",
    "    ds_train = build_dataset(train_paths, y_train, training=True)\n",
    "    ds_val = build_dataset(val_paths, y_val, training=False)\n",
    "\n",
    "    # 5) Build model\n",
    "    num_classes = len(label2idx)\n",
    "    pen_to_pixel = build_crnn_model(num_classes=num_classes, input_shape=(*IMG_SIZE, 1))\n",
    "    pen_to_pixel.summary()\n",
    "\n",
    "    # 6) Callbacks: checkpoint + early stopping + reduce LR\n",
    "    dirname = os.path.dirname(BEST_MODEL_PATH)\n",
    "    if dirname:\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "    ckpt = keras.callbacks.ModelCheckpoint(\n",
    "        BEST_MODEL_PATH,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,  # save full model in .keras (zip) format\n",
    "        mode=\"max\",\n",
    "        verbose=1,\n",
    "    )\n",
    "    early = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=6,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    rlr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # 7) Train\n",
    "    history = pen_to_pixel.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[ckpt, early, rlr],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # 8) Save final (best already saved via checkpoint)\n",
    "    pen_to_pixel.save(FINAL_MODEL_PATH)\n",
    "    print(f\"Saved final model to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "    # 9) Evaluate on validation set\n",
    "    val_metrics = pen_to_pixel.evaluate(ds_val, verbose=1)\n",
    "    print(\"Validation metrics:\", dict(zip(pen_to_pixel.metrics_names, val_metrics)))\n",
    "\n",
    "    # 10) Save label maps (critical for decoding predictions later)\n",
    "    with open(LABELMAP_JSON, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"label2idx\": label2idx,\n",
    "                \"idx2label\": {int(k): v for k, v in idx2label.items()},\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "    print(f\"Saved label maps to {LABELMAP_JSON}\")\n",
    "\n",
    "    # 11) Quick demo predictions on a few validation images\n",
    "    print(\"Demo predictions (first 5 validation images):\")\n",
    "    sample_paths = val_paths[:5]\n",
    "    for p in sample_paths:\n",
    "        pred_idx, conf = predict_single_image(pen_to_pixel, p)\n",
    "        print(f\"{Path(p).name} -> pred={idx2label[pred_idx]}  conf={conf:.3f}\")\n",
    "\n",
    "\n",
    "def predict_single_image(model: keras.Model, path: str) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Loads one image path, returns (pred_class_index, confidence).\n",
    "    \"\"\"\n",
    "    img = load_and_preprocess_image(tf.constant(path))\n",
    "    img = tf.expand_dims(img, axis=0)  # (1, H, W, 1)\n",
    "    probs = model.predict(img, verbose=0)[0]\n",
    "    pred_idx = int(tf.argmax(probs).numpy())\n",
    "    conf = float(tf.reduce_max(probs).numpy())\n",
    "    return pred_idx, conf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af87842-b4f3-4866-b0be-08b014cd7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd19_pen_to_pixel.py\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image  # noqa: F401  # (kept so you can sanity-check images if needed)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from tensorflow.keras import mixed_precision\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  \n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "# only show warnings and errors\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "DATASET_DIR = \"/home/mudda/Downloads/by_merge\"   # <-- change to your SD19 by_class path\n",
    "IMG_SIZE = (128, 128)                            # (H, W)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "VAL_SPLIT = 0.15\n",
    "RANDOM_SEED = 42\n",
    "SHUFFLE_BUFFER = 10_000\n",
    "\n",
    "# Save names\n",
    "BEST_MODEL_PATH = \"pen_to_pixel.keras\"           # best checkpoint (.keras zip)\n",
    "FINAL_MODEL_PATH = \"pen_to_pixel.keras\"    # final save after training\n",
    "LABELMAP_JSON = \"pen_to_pixel_labelmap.json\"     # label maps for inference\n",
    "\n",
    "# Acceptable image extensions in typical SD19 dumps (skip .mit etc.)\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\", \".gif\"}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Reproducibility niceties\n",
    "# =========================\n",
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Dataset utilities\n",
    "# =========================\n",
    "def is_image_file(p: str) -> bool:\n",
    "    return Path(p).suffix.lower() in IMG_EXTS\n",
    "\n",
    "\n",
    "def scan_dataset(by_class_dir: str) -> Tuple[List[str], List[str], Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Scans SD19 by_class directory structure:\n",
    "      by_class/\n",
    "        <class_name_or_id>/\n",
    "          hsf_1/*.png ...\n",
    "          hsf_2/*.png ...\n",
    "    Returns:\n",
    "      filepaths: list of image file paths\n",
    "      str_labels: list of string labels (top-level dir names)\n",
    "      label2idx: mapping {label_str -> int}\n",
    "      idx2label: reverse mapping\n",
    "    \"\"\"\n",
    "    filepaths: List[str] = []\n",
    "    str_labels: List[str] = []\n",
    "\n",
    "    label_dirs = sorted([d for d in glob.glob(os.path.join(by_class_dir, \"*\")) if os.path.isdir(d)])\n",
    "    if not label_dirs:\n",
    "        raise RuntimeError(f\"No class folders found under: {by_class_dir}\")\n",
    "\n",
    "    label_names = [os.path.basename(d) for d in label_dirs]\n",
    "    label2idx = {lab: i for i, lab in enumerate(label_names)}\n",
    "    idx2label = {i: lab for lab, i in label2idx.items()}\n",
    "\n",
    "    for lab_dir in label_dirs:\n",
    "        lab = os.path.basename(lab_dir)\n",
    "        for p in glob.glob(os.path.join(lab_dir, \"**\", \"*\"), recursive=True):\n",
    "            if os.path.isfile(p) and is_image_file(p):\n",
    "                filepaths.append(p)\n",
    "                str_labels.append(lab)\n",
    "\n",
    "    if len(filepaths) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No image files found. Ensure DATASET_DIR points to SD19 'by_class' root \"\n",
    "            \"and that image files (e.g., .png) exist.\"\n",
    "        )\n",
    "\n",
    "    return filepaths, str_labels, label2idx, idx2label\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(path: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Reads an image file, converts to grayscale, resizes to IMG_SIZE, scales to [0,1].\n",
    "    Returns shape (H, W, 1).\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img, channels=1, expand_animations=False)  # grayscale\n",
    "    # Ensure (H, W, 1) static rank\n",
    "    img.set_shape([None, None, 1])\n",
    "    img = tf.image.resize(img, IMG_SIZE, method=tf.image.ResizeMethod.BILINEAR)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def build_dataset(paths: List[str], labels: np.ndarray, training: bool) -> tf.data.Dataset:\n",
    "    ds_paths = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    ds_labels = tf.data.Dataset.from_tensor_slices(labels.astype(np.int32))\n",
    "    ds = tf.data.Dataset.zip((ds_paths, ds_labels))\n",
    "\n",
    "    def _map_fn(p, y):\n",
    "        img = load_and_preprocess_image(p)\n",
    "        return img, y\n",
    "\n",
    "    ds = ds.map(_map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(SHUFFLE_BUFFER, seed=RANDOM_SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Model: CNN + BiLSTM (CRNN)\n",
    "# =========================\n",
    "def build_crnn_model(num_classes: int, input_shape=(128, 128, 1)) -> keras.Model:\n",
    "    \"\"\"\n",
    "    CRNN for single-character classification.\n",
    "    - CNN extracts feature map (B, H', W', C')\n",
    "    - Reshape into a sequence across width -> BiLSTM\n",
    "    - Dense softmax for class logits\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=input_shape, name=\"image\")\n",
    "\n",
    "    # CNN feature extractor\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)  # -> 64x64\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)  # -> 32x32\n",
    "\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)  # -> 16x16\n",
    "\n",
    "    x = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # Keep 16x16 to have a decent sequence length (width=16)\n",
    "\n",
    "    # Prepare sequence: (B, H, W, C) -> (B, W, H*C)\n",
    "    # Use dynamic shape to be safe\n",
    "    x = layers.Permute((2, 1, 3))(x)  # (B, W, H, C)\n",
    "    x = layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2] * tf.shape(t)[3]]))(x)\n",
    "\n",
    "    # RNN\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=False))(x)  # -> (B, 512)\n",
    "\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    pen_to_pixel = keras.Model(inp, out, name=\"pen_to_pixel\")\n",
    "    pen_to_pixel.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return pen_to_pixel\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Train / Eval\n",
    "# =========================\n",
    "def main():\n",
    "    set_all_seeds(RANDOM_SEED)\n",
    "\n",
    "    # Optional GPU memory growth\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 1) Scan dataset\n",
    "    print(f\"ğŸ” Scanning dataset at: {DATASET_DIR}\")\n",
    "    filepaths, str_labels, label2idx, idx2label = scan_dataset(DATASET_DIR)\n",
    "    print(f\"Found {len(filepaths)} images across {len(label2idx)} classes.\")\n",
    "    print(\"Example classes:\", list(label2idx.keys())[:10])\n",
    "\n",
    "    # 2) Convert string labels to class indices\n",
    "    y = np.array([label2idx[s] for s in str_labels], dtype=np.int32)\n",
    "\n",
    "    # 3) Stratified train/val split\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SPLIT, random_state=RANDOM_SEED)\n",
    "    train_idx, val_idx = next(sss.split(filepaths, y))\n",
    "    train_paths = [filepaths[i] for i in train_idx]\n",
    "    val_paths = [filepaths[i] for i in val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    print(f\"Train samples: {len(train_paths)} | Val samples: {len(val_paths)}\")\n",
    "\n",
    "    # 4) Build tf.data pipelines (with shuffle for training)\n",
    "    ds_train = build_dataset(train_paths, y_train, training=True)\n",
    "    ds_val = build_dataset(val_paths, y_val, training=False)\n",
    "\n",
    "    # 5) Build model\n",
    "    num_classes = len(label2idx)\n",
    "    pen_to_pixel = build_crnn_model(num_classes=num_classes, input_shape=(*IMG_SIZE, 1))\n",
    "    pen_to_pixel.summary()\n",
    "\n",
    "    # 6) Callbacks: checkpoint + early stopping + reduce LR\n",
    "    dirname = os.path.dirname(BEST_MODEL_PATH)\n",
    "    if dirname:\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "    ckpt = keras.callbacks.ModelCheckpoint(\n",
    "        BEST_MODEL_PATH,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,  # save full model in .keras (zip) format\n",
    "        mode=\"max\",\n",
    "        verbose=1,\n",
    "    )\n",
    "    early = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=6,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    rlr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # 7) Train\n",
    "    history = pen_to_pixel.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[ckpt, early, rlr],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # 8) Save final (best already saved via checkpoint)\n",
    "    pen_to_pixel.save(FINAL_MODEL_PATH)\n",
    "    print(f\"Saved final model to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "    # 9) Evaluate on validation set\n",
    "    val_metrics = pen_to_pixel.evaluate(ds_val, verbose=1)\n",
    "    print(\"Validation metrics:\", dict(zip(pen_to_pixel.metrics_names, val_metrics)))\n",
    "\n",
    "    # 10) Save label maps (critical for decoding predictions later)\n",
    "    with open(LABELMAP_JSON, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"label2idx\": label2idx,\n",
    "                \"idx2label\": {int(k): v for k, v in idx2label.items()},\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "    print(f\"Saved label maps to {LABELMAP_JSON}\")\n",
    "\n",
    "    # 11) Quick demo predictions on a few validation images\n",
    "    print(\"Demo predictions (first 5 validation images):\")\n",
    "    sample_paths = val_paths[:5]\n",
    "    for p in sample_paths:\n",
    "        pred_idx, conf = predict_single_image(pen_to_pixel, p)\n",
    "        print(f\"{Path(p).name} -> pred={idx2label[pred_idx]}  conf={conf:.3f}\")\n",
    "\n",
    "\n",
    "def predict_single_image(model: keras.Model, path: str) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Loads one image path, returns (pred_class_index, confidence).\n",
    "    \"\"\"\n",
    "    img = load_and_preprocess_image(tf.constant(path))\n",
    "    img = tf.expand_dims(img, axis=0)  # (1, H, W, 1)\n",
    "    probs = model.predict(img, verbose=0)[0]\n",
    "    pred_idx = int(tf.argmax(probs).numpy())\n",
    "    conf = float(tf.reduce_max(probs).numpy())\n",
    "    return pred_idx, conf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c3015-b8df-4310-a4c1-efab8f182a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
